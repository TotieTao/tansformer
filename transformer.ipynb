{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b4a952",
   "metadata": {},
   "source": [
    "# 1、Data 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c96a8cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zidian_x: {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'q': 13, 'w': 14, 'e': 15, 'r': 16, 't': 17, 'y': 18, 'u': 19, 'i': 20, 'o': 21, 'p': 22, 'a': 23, 's': 24, 'd': 25, 'f': 26, 'g': 27, 'h': 28, 'j': 29, 'k': 30, 'l': 31, 'z': 32, 'x': 33, 'c': 34, 'v': 35, 'b': 36, 'n': 37, 'm': 38}\n",
      " zidian_y: {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'Q': 13, 'W': 14, 'E': 15, 'R': 16, 'T': 17, 'Y': 18, 'U': 19, 'I': 20, 'O': 21, 'P': 22, 'A': 23, 'S': 24, 'D': 25, 'F': 26, 'G': 27, 'H': 28, 'J': 29, 'K': 30, 'L': 31, 'Z': 32, 'X': 33, 'C': 34, 'V': 35, 'B': 36, 'N': 37, 'M': 38}\n"
     ]
    }
   ],
   "source": [
    "## Data\n",
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "\n",
    "zidian_x = '<SOS>,<EOS>,<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m' #定义所有字符集\n",
    "zidian_x = {word: i for i, word in enumerate(zidian_x.split(','))} #将字符集做成字典和索引\n",
    "zidian_xr = [word for word,i in zidian_x.items()] #zidian_x包含的字符\n",
    "\n",
    "zidian_y = {Word.upper():i for Word, i in zidian_x.items()}\n",
    "zidian_yr = [word for word, i in zidian_y.items()]\n",
    "\n",
    "print(\"zidian_x: {}\\n zidian_y: {}\".format(zidian_x,zidian_y))\n",
    "\n",
    "#获得句子和翻译后的句子，思路是将输入的句子逆序，同时小写字母翻译成大写字母，数字翻译成10-i\n",
    "def get_data():\n",
    "    #定义词集合\n",
    "    words = [\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'q', 'w', 'e', 'r',\n",
    "        't', 'y', 'u', 'i', 'o', 'p', 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k',\n",
    "        'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm'\n",
    "    ]\n",
    "    \n",
    "    #定义每个词出现的频率,其实也可以随机生成\n",
    "#     p = np.array([\n",
    "#         1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
    "#         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26\n",
    "#     ])\n",
    "    p = np.random.rand(1,36)\n",
    "    p = p / p.sum()\n",
    "    \n",
    "    #随机选n个词组成句子\n",
    "    n = np.random.randint(30, 48) #生成长度为30-48个词的句子\n",
    "    x = np.random.choice(words, size = n, replace=True,p=p[0])\n",
    "    x = x.tolist()\n",
    "    \n",
    "    #字母小写转大写，数字10-i\n",
    "    y = []\n",
    "    for k in x:\n",
    "        if k.isdigit():\n",
    "            y.append(str(9-int(k)))\n",
    "        else:\n",
    "            y.append(k.upper())\n",
    "                  \n",
    "    y = y + [y[-1]] #认为给y增加一个字符，使y的维度是x+1，目的是在后面预测字符时能用到第一个字符（）这部分后续理解再说\n",
    "    \n",
    "    #逆序\n",
    "    y = y[::-1]\n",
    "    \n",
    "    #加上首位符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "    \n",
    "    #补固定长度pad\n",
    "    x = x + ['<PAD>']*50\n",
    "    y = y + ['<PAD>']*51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "    \n",
    "    #编码成索引数据\n",
    "    x = [zidian_x[k] for k in x]\n",
    "    y = [zidian_y[k] for k in y]\n",
    "    \n",
    "    #转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)                     \n",
    "    \n",
    "    return x,y\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return get_data()\n",
    "\n",
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=Dataset(),\n",
    "                                     batch_size=8,\n",
    "                                     drop_last=True,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=None)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd889ff7",
   "metadata": {},
   "source": [
    "### 生成一个句子编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494efd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 10, 20,  8, 31,  6, 35, 34, 20, 23, 37, 16, 29, 18, 37, 13, 27, 31,\n",
      "        31, 33, 27, 23,  7, 29, 25,  8, 33, 29, 38, 38,  8, 29, 20,  6,  8, 35,\n",
      "         7, 35, 19,  9,  9, 24,  1,  2,  2,  2,  2,  2,  2,  2]) tensor([ 0, 24, 24,  6,  6, 19, 35,  8, 35,  7,  9, 20, 29,  7, 38, 38, 29, 33,\n",
      "         7, 25, 29,  8, 23, 27, 33, 31, 31, 27, 13, 37, 18, 29, 16, 37, 23, 20,\n",
      "        34, 35,  9, 31,  7, 20,  5,  1,  2,  2,  2,  2,  2,  2,  2])\n"
     ]
    }
   ],
   "source": [
    "wx,wy = get_data()\n",
    "print(wx,wy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a33d74",
   "metadata": {},
   "source": [
    "# 2、功能 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1701989",
   "metadata": {},
   "source": [
    "### Mask 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f56ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask的单列pad设置\n",
    "def mask_pad(data):\n",
    "    #此时句子还未embed\n",
    "    #data = [b, 50] b句话50词\n",
    "    #判断每个词是不是<PAD>\n",
    "    mask = data == zidian_x['<PAD>']\n",
    "    \n",
    "    # [b, 50] -> [b, 1, 1, 50] \n",
    "    # 有什么用？现在是每句话有一个1*50的向量表示，在后面对每句话里词与词之间进行词间注意力计算需要生成50*50的词矩阵，\n",
    "    # 第2个一是在多头注意力模块中会分成4个头，Q*K得到的score的数据格式是[b,4,50,50]（可继续往下看），因此为了对齐多设置一维\n",
    "    \n",
    "    mask = mask.reshape(-1, 1, 1, 50)\n",
    "    \n",
    "    # 在计算注意力时,是计算50个词和50个词相互之间的注意力,所以是个50*50的矩阵\n",
    "    # 是pad的列是true,意味着任何词对pad的注意力都是0\n",
    "    # 但是pad本身对其他词的注意力并不是0，且防止归一化计算出错\n",
    "    # 所以是pad的行不是true\n",
    "    \n",
    "    # 复制n次\n",
    "    # [b, 1, 1, 50] -> [b, 1, 50, 50]\n",
    "    mask = mask.expand(-1, 1, 50, 50)\n",
    "    return mask\n",
    "\n",
    "#上三角pad设置，这部分是decoder预测中防止信息泄露用\n",
    "def mask_tril(data):\n",
    "    # 50*50的矩阵表示每个词对其他词是否可见\n",
    "    # 上三角矩阵,不包括对角线,意味着,对每个词而言,他只能看到他自己,和他之前的词,而看不到之后的词\n",
    "    # [1, 50, 50]\n",
    "    \"\"\"\n",
    "    [[0, 1, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 0, 1, 1],\n",
    "     [0, 0, 0, 0, 1],\n",
    "     [0, 0, 0, 0, 0]]\"\"\"\n",
    "    \n",
    "    tril = 1- torch.tril(torch.ones(1,50,50, dtype=torch.long))\n",
    "     # 判断y当中每个词是不是pad,如果是pad则不可见\n",
    "    # [b, 50]\n",
    "    mask = data == zidian_y['<PAD>']\n",
    "   \n",
    "    # 变形+转型,为了之后的计算\n",
    "    # [b, 1, 50]\n",
    "    mask = mask.unsqueeze(1).long()\n",
    "    \n",
    "    # mask和tril求并集\n",
    "    # [b, 1, 50] + [1, 50, 50] -> [b, 50, 50]\n",
    "    mask = mask + tril\n",
    "    \"\"\"\n",
    "    [[0, 1, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 1, 0, 1],\n",
    "     [0, 0, 1, 0, 0]] \n",
    "    \"\"\"\n",
    "    \n",
    "    # 转布尔型\n",
    "    mask = mask > 0\n",
    "\n",
    "    # 转布尔型,增加一个维度,便于后续的计算,[b, 50, 50]->[b, 1, 50, 50]\n",
    "    mask = (mask == 1).unsqueeze(dim=1)\n",
    "\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b970290",
   "metadata": {},
   "source": [
    "### multi-head attention 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de106a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, mask):\n",
    "    #QKV = [b,4,50,8]\n",
    "    #b 句话，每句话50词，每个词编码32维向量，4个头，每个头分为8维向量\n",
    "    # Q * K = [b,4,50,8] * [b,4,8,50] -> [b, 4, 50,50]\n",
    "    score = torch.matmul(Q, K.permute(0, 1, 3, 2)) #得到50*50的注意力得分矩阵，表示每个词与其他词之间的相似度或重要性\n",
    "    \n",
    "    # 除以每个头维数的平方根,做数值缩放，防止内积过大\n",
    "    score /= 8 ** 0.5\n",
    "    \n",
    "    # mask遮盖,mask是true的地方都被替换成-inf,这样在计算softmax的时候,-inf会被压缩到0\n",
    "    # mask = [b, 1, 50, 50]\n",
    "    score = score.masked_fill_(mask, -float('inf'))\n",
    "    score = torch.softmax(score, dim=-1) # softmax对score归一化后mask的位置变为0,\n",
    "    \n",
    "    #S*V 得到每个词相对其他词的注意力值\n",
    "    #[b,4,50,50]*[b,4,50,8] -> [b,4,50,8]\n",
    "    score = torch.matmul(score, V)\n",
    "    \n",
    "    #将4个头合并\n",
    "    #[b,4,50,8] -> [b,50,32]\n",
    "    score = score.permute(0,2,1,3).reshape(-1,50,32)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de153c09",
   "metadata": {},
   "source": [
    "1、先计算Q*K得到50*50的注意力得分矩阵，表示每个词与其他词之间的相似度或重要性\n",
    "\n",
    "2、mask矩阵也是50*50, 将其中true的位置在score矩阵中替换，保证<PAD>补句子的部分不参与注意力计算\n",
    "    \n",
    "3、计算S*V得到最后的attention表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd8f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_Q = torch.nn.Linear(32,32) #每个词32维\n",
    "        self.fc_K = torch.nn.Linear(32,32)\n",
    "        self.fc_V = torch.nn.Linear(32,32)\n",
    "        \n",
    "        self.out_fc = torch.nn.Linear(32,32)\n",
    "        \n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32, elementwise_affine = True)#标准化\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask):\n",
    "        #b句话，每句50词，每个词编成32维向量\n",
    "        #有Q,K,V = [b,50,32]\n",
    "\n",
    "        b = Q.shape[0]\n",
    "        \n",
    "        #保留原始Q，后续短接用，（为什么只有Q？）\n",
    "        clone_Q = Q.clone()\n",
    "        \n",
    "        #规范化，这块论文中是在线性运算后进行，但实验证明放在之前效果会更好\n",
    "        Q = self.norm(Q)\n",
    "        K = self.norm(K)\n",
    "        V = self.norm(V)\n",
    "        \n",
    "        #线性运算，维度不变\n",
    "        K = self.fc_K(K)\n",
    "        V = self.fc_V(V)\n",
    "        Q = self.fc_Q(Q)\n",
    "        \n",
    "        #拆分多头，32维分4头拆分维8维\n",
    "        #[b,50,32] -> [b,4,50,8]\n",
    "        Q = Q.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #计算注意力\n",
    "        #[b,4,50,8] -> [b,50,32]\n",
    "        score = attention(Q, K, V, mask)\n",
    "        \n",
    "        # 计算输出,维度不变\n",
    "        # [b, 50, 32] -> [b, 50, 32]\n",
    "        score = self.dropout(self.out_fc(score)) #防止过拟合\n",
    "        # 短接\n",
    "        score = clone_Q + score\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68813f1a",
   "metadata": {},
   "source": [
    "### 位置编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7f3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #pos是第几个词，i是第几个维度，d_model是维度总数\n",
    "        def get_pe(pos, i, d_model):\n",
    "            fenmu = 1e4**(i/d_model)\n",
    "            pe = pos / fenmu\n",
    "            \n",
    "            if i%2 == 0:\n",
    "                return math.sin(pe)\n",
    "            return math.cos(pe)\n",
    "        \n",
    "        #初始化位置编码矩阵\n",
    "        pe = torch.empty(50,32)\n",
    "        for i in range(50):\n",
    "            for j in range(32):\n",
    "                pe[i,j] = get_pe(i, j, 32)\n",
    "        pe = pe.unsqueeze(0) #给矩阵添加一个中括号升维\n",
    "        \n",
    "        #定义为不更新的常量\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "        #词编码层\n",
    "        self.embed = torch.nn.Embedding(39, 32) #zidian_x所有的词有39个词，编码位置有39个且为32维\n",
    "        #初始化参数\n",
    "        self.embed.weight.data.normal_(0, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #[b, 50] -> [b, 50, 32]\n",
    "        embed = self.embed(x) #将b句话传进来进行词编码\n",
    "        \n",
    "        #词编码和位置编码相加\n",
    "        #[b,50,32]+[1,50,32] -> [b,50,32]\n",
    "        embed = embed + self.pe\n",
    "        \n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126012f7",
   "metadata": {},
   "source": [
    "### 全连接层 feed_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33958e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedOutput(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=32, out_features=64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "        )\n",
    "        \n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,\n",
    "                                       elementwise_affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        clone_x = x.clone()\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        #全连接运算\n",
    "        #[b,50,32] -> [b,50,32]\n",
    "        out = self.fc(x)\n",
    "\n",
    "        #短接\n",
    "        out = clone_x + out\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eb5ed",
   "metadata": {},
   "source": [
    "# 3、model模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbeff7",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d78d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mh = MultiHead()\n",
    "        self.fc = FullyConnectedOutput()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 计算自注意力,维度不变\n",
    "        # [b, 50, 32] -> [b, 50, 32]\n",
    "        score = self.mh(x, x, x, mask)\n",
    "        \n",
    "        # 全连接输出,维度不变\n",
    "        # [b, 50, 32] -> [b, 50, 32]\n",
    "        out = self.fc(score)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = EncoderLayer()\n",
    "        self.layer_2 = EncoderLayer()\n",
    "        self.layer_3 = EncoderLayer()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_1(x, mask)\n",
    "        x = self.layer_2(x, mask)\n",
    "        x = self.layer_3(x, mask)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae138d",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a1b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mh1 = MultiHead()\n",
    "        self.mh2 = MultiHead()\n",
    "        self.fc = FullyConnectedOutput()\n",
    "        \n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y ):\n",
    "        # 先计算y的自注意力,维度不变\n",
    "        # [b, 50, 32] -> [b, 50, 32]\n",
    "\n",
    "        y = self.mh1(y, y, y, mask_tril_y)\n",
    "        # 结合x和y的注意力计算,维度不变\n",
    "        # [b, 50, 32],[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.mh2(y, x, x, mask_pad_x)\n",
    "        # 全连接输出,维度不变\n",
    "        # [b, 50, 32] -> [b, 50, 32]\n",
    "        out = self.fc(y)\n",
    "\n",
    "        return out\n",
    "        \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = DecoderLayer()\n",
    "        self.layer_2 = DecoderLayer()\n",
    "        self.layer_3 = DecoderLayer()\n",
    "        \n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        y = self.layer_1(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_2(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_3(x, y, mask_pad_x, mask_tril_y)\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c7122",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b35590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_x = PositionEmbedding()\n",
    "        self.embed_y = PositionEmbedding()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.fc_out = torch.nn.Linear(32,39)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        #[b, 1, 50, 50]\n",
    "        mask_pad_x = mask_pad(x)\n",
    "        mask_tril_y = mask_tril(y)\n",
    "        \n",
    "        #embedding编码，添加位置信息\n",
    "        # x = [b, 50] -> [b, 50, 32]\n",
    "        # y = [b, 50] -> [b, 50, 32]\n",
    "        x, y = self.embed_x(x), self.embed_y(y)\n",
    "        \n",
    "        #encoder\n",
    "        x = self.encoder(x, mask_pad_x)\n",
    "        \n",
    "        #decoder\n",
    "        y = self.decoder(x, y, mask_pad_x, mask_tril_y)\n",
    "        \n",
    "        #全连接输出\n",
    "        #[b,50,32] -> [b,50,39]        \n",
    "        y = self.fc_out(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f0101",
   "metadata": {},
   "source": [
    "# 5、main测试 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee9edf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.002 3.968126058578491 0.02476780185758514\n",
      "y:tensor([34, 34, 21, 23, 35, 26,  5, 26,  6, 16, 36, 35, 21, 23, 19, 32, 23, 23,\n",
      "        11,  5, 23, 17,  5, 37, 12, 29, 28,  5, 21, 19, 21,  7,  1,  7,  7, 23,\n",
      "         5,  8, 17, 15, 33, 29, 16, 22, 12, 35, 16, 24, 30, 11,  8, 28,  8, 11,\n",
      "        37, 26, 10, 36,  7,  4, 35, 26, 11,  6, 14, 21,  6, 32, 37, 14, 38, 32,\n",
      "         9, 37, 15, 24, 26, 27, 13,  1,  4,  4,  9,  9, 13,  9, 29, 10, 13, 12,\n",
      "        23, 10, 28, 20, 34,  6, 35, 22, 18, 21, 20,  9, 35, 10, 30,  4, 36, 28,\n",
      "        18, 37, 22, 18, 38, 38, 21, 18,  4,  1, 14, 14, 23, 33, 11, 30, 14, 16,\n",
      "        14, 10, 22, 17, 24, 11, 10, 18, 22,  4, 35, 11, 24, 26, 21, 22, 17,  4,\n",
      "         3, 21, 20, 26, 11, 26, 13, 14, 37, 28, 19, 20,  3, 17, 23,  1, 32, 32,\n",
      "        36, 16, 21, 32, 36, 33, 24, 27,  6,  9,  8, 24, 16, 25, 33, 20, 37, 19,\n",
      "         9, 10, 32, 23, 20,  3, 17,  6, 13,  4,  7, 13, 32, 20, 16, 12, 28,  8,\n",
      "        18,  6, 18, 26, 28, 24,  1, 11, 11, 23, 15, 26,  3,  4, 32, 14, 35, 26,\n",
      "        25, 18, 13, 13,  5, 34, 26, 21, 18,  5, 33, 14, 23, 35, 25, 12, 11, 25,\n",
      "        27, 23, 21, 35, 19, 20, 24, 35, 13, 27, 27, 29, 11, 33, 13, 32,  8, 15,\n",
      "         1, 17, 17, 26, 13, 30, 27, 16, 17, 23, 36, 34, 23, 36, 17, 21,  4, 21,\n",
      "        21,  4,  5, 33, 32, 25, 38, 15, 21, 34, 12, 13, 13,  5, 36,  1, 21, 21,\n",
      "        19, 24, 18, 28, 13, 14, 26, 30, 25, 28, 31, 26, 24, 11, 36, 18, 37, 27,\n",
      "         6, 21,  6, 34, 20,  6,  8,  6, 20, 18, 15, 38,  3,  8, 18, 23,  1])\n",
      "pred:tensor([ 0,  4,  4,  4,  0,  4,  4,  4, 38,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  3, 16,  4,  4,  4,  4,  4,  4,  0,  4,  4,  4,  0,  4,  4,\n",
      "         4,  4,  4,  4, 38,  4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,\n",
      "         4,  4,  4,  2,  4,  4,  4,  4,  4,  4,  4,  4,  0,  0,  0,  0,  0,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  0,  4,  4,  4,  4,  4,  4,  4,  0,  4,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  3,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  0,  0,  0,  4,  0,  4,  4,  0,  0,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  3,  0,  4,  4,  4,\n",
      "         3,  4, 14,  4,  4,  4,  0,  0,  0,  0,  4,  4,  0,  4,  0,  4,  4,  0,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0, 15,  4, 16,  4,  4,  4,  4,  4,  4,  4,  4,  0,  0,  0,  0,  4,  4,\n",
      "         0,  0,  4,  4,  4,  4,  4,  4,  4,  4,  0,  0,  4,  4,  4, 15,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0, 12,  4, 16,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  0,  0,  0,  4,  0, 12,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 38,  4,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  3,  4,  4,  4,  4,  4,  4,  4,  3,  4,  0,  4,  0,  4,\n",
      "         0,  4,  4,  4,  4,  4,  4,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,  4,  4,  4,  0,  0],\n",
      "       grad_fn=<NotImplemented>) \n",
      "sum:8\n",
      "0 200 0.002 3.571964979171753 0.05362776025236593\n",
      "0 400 0.002 3.5131266117095947 0.06481481481481481\n",
      "0 600 0.002 3.4009766578674316 0.08069164265129683\n",
      "0 800 0.002 2.993008852005005 0.19571865443425077\n",
      "0 1000 0.002 1.9853471517562866 0.3667711598746082\n",
      "0 1200 0.002 1.231961727142334 0.5751633986928104\n",
      "0 1400 0.002 1.0696974992752075 0.6702508960573477\n",
      "0 1600 0.002 0.8739535212516785 0.6909090909090909\n",
      "0 1800 0.002 0.6285437345504761 0.7974683544303798\n",
      "0 2000 0.002 0.832805335521698 0.7286135693215339\n",
      "0 2200 0.002 0.6492680311203003 0.7980769230769231\n",
      "0 2400 0.002 0.4487563967704773 0.8730650154798761\n",
      "0 2600 0.002 0.431328147649765 0.8463855421686747\n",
      "0 2800 0.002 0.22960080206394196 0.9276315789473685\n",
      "0 3000 0.002 0.45117872953414917 0.8303030303030303\n",
      "0 3200 0.002 0.25523385405540466 0.9225806451612903\n",
      "0 3400 0.002 0.2176593840122223 0.9421221864951769\n",
      "0 3600 0.002 0.3700020909309387 0.8641618497109826\n",
      "0 3800 0.002 0.18252751231193542 0.94\n",
      "0 4000 0.002 0.2725400924682617 0.9161676646706587\n",
      "0 4200 0.002 0.24453139305114746 0.9390243902439024\n",
      "0 4400 0.002 0.3773435056209564 0.891156462585034\n",
      "0 4600 0.002 0.32398808002471924 0.9129129129129129\n",
      "0 4800 0.002 0.10862614214420319 0.9617834394904459\n",
      "0 5000 0.002 0.10317105054855347 0.9808306709265175\n",
      "0 5200 0.002 0.14977869391441345 0.9470404984423676\n",
      "0 5400 0.002 0.07081642746925354 0.974025974025974\n",
      "0 5600 0.002 0.06168273836374283 0.9810126582278481\n",
      "0 5800 0.002 0.11132786422967911 0.9723926380368099\n",
      "0 6000 0.002 0.1876579225063324 0.9369085173501577\n",
      "0 6200 0.002 0.1673635095357895 0.945619335347432\n",
      "0 6400 0.002 0.2730870544910431 0.9365558912386707\n",
      "0 6600 0.002 0.11325178295373917 0.9714285714285714\n",
      "0 6800 0.002 0.06882044672966003 0.9804560260586319\n",
      "0 7000 0.002 0.4311251640319824 0.8871473354231975\n",
      "0 7200 0.002 0.10835961997509003 0.9712643678160919\n",
      "0 7400 0.002 0.05345604941248894 0.9835526315789473\n",
      "0 7600 0.002 0.1302703619003296 0.9768211920529801\n",
      "0 7800 0.002 0.04285430535674095 0.9882697947214076\n",
      "0 8000 0.002 0.11476495116949081 0.9676470588235294\n",
      "0 8200 0.002 0.26314082741737366 0.9355828220858896\n",
      "0 8400 0.002 0.04010498523712158 0.9910714285714286\n",
      "0 8600 0.002 0.07403497397899628 0.985207100591716\n",
      "0 8800 0.002 0.018597658723592758 0.9967320261437909\n",
      "0 9000 0.002 0.0935794934630394 0.9735294117647059\n",
      "0 9200 0.002 0.05422841012477875 0.9932432432432432\n",
      "0 9400 0.002 0.06449422985315323 0.9807692307692307\n",
      "0 9600 0.002 0.05939869582653046 0.9847560975609756\n",
      "0 9800 0.002 0.0769910141825676 0.9727272727272728\n",
      "0 10000 0.002 0.09811019897460938 0.9611650485436893\n",
      "y:tensor([12, 12, 25, 28, 15,  8, 18, 26,  4, 33, 33, 33, 12, 17,  4, 12, 12,  6,\n",
      "        32, 14, 33, 35, 26,  7, 18, 24, 37, 33, 27, 15,  7, 28, 10, 19, 26, 34,\n",
      "         4, 19, 22, 34,  4,  1, 12, 12, 19, 34, 11, 37,  4, 13, 28, 17, 13, 16,\n",
      "        32, 26,  3,  3, 18, 11, 27, 10, 26, 33, 29, 16, 11, 22,  3, 16, 21, 13,\n",
      "        28, 33, 19, 36, 29,  4, 28,  1,  6,  6, 22, 19, 23, 21,  6,  5, 20,  6,\n",
      "        24,  5, 10, 29, 35, 10, 31, 35,  4, 14, 27, 27, 19,  6,  9, 13, 25, 20,\n",
      "        25, 10, 16,  6,  6,  1,  3,  3, 24, 15, 30,  8, 24, 20, 20, 10, 31, 30,\n",
      "        14,  9,  3, 22, 26, 26, 19, 26, 10, 21, 25,  6, 37, 33, 13, 28, 26, 16,\n",
      "        26, 31, 15, 37, 10, 38, 25, 14,  1,  8,  8, 16, 12, 16, 25,  8, 31, 28,\n",
      "        35, 35,  7, 12, 14, 34, 13, 23, 28, 16, 15,  9, 35, 22, 29, 30, 11, 33,\n",
      "        24,  8, 35, 34, 29, 25, 30, 10, 33, 38, 15, 20, 28, 37, 35,  3,  8, 35,\n",
      "         1, 31, 31, 33, 26, 31, 16,  5, 12, 35, 29, 17, 38, 11, 32,  6, 26, 29,\n",
      "        25,  5, 34,  3, 17, 18,  6, 12, 17,  6, 26, 35, 10, 11, 16, 23, 35,  5,\n",
      "         6, 26, 37, 14,  1, 28, 28, 16, 28, 28, 13,  8, 14, 18, 23,  3, 12, 36,\n",
      "         4, 25, 31, 19, 10, 31, 13, 13, 10, 12, 30, 16, 26, 18, 31,  3, 27, 11,\n",
      "         1, 30, 30,  6,  4, 33,  7,  3, 19, 32, 17, 17,  7, 30, 10,  7, 10, 17,\n",
      "        13,  9,  9, 27, 32, 19,  4, 10, 34, 36, 14, 17, 34, 22, 27,  9, 17, 22,\n",
      "        20, 35,  1])\n",
      "pred:tensor([12, 12, 25, 28, 15, 15, 18, 26,  4, 33, 33, 33, 12, 17,  4, 12, 12,  6,\n",
      "        32,  6, 33, 35, 26,  7, 18, 24, 37, 33, 27, 15,  7, 28, 10, 19, 26, 34,\n",
      "         4, 19, 22, 34,  4,  1, 12, 12, 19, 34, 11, 37,  4, 13, 28, 17, 13, 16,\n",
      "        32, 26,  3,  3, 18, 11, 27, 10, 26, 33, 29, 16, 11, 22,  3, 16, 21, 13,\n",
      "        28, 33, 19, 36, 29,  4, 28,  1,  6,  6, 22, 19, 23, 21,  6,  5, 20,  6,\n",
      "        24,  5, 10, 29, 35, 10, 31, 35,  4, 14, 27, 27, 19,  6,  9, 13, 25, 20,\n",
      "        25, 10, 16,  6,  6,  1,  3,  3, 24, 15, 30,  8, 24, 20, 20, 10, 31, 30,\n",
      "        14,  9, 14, 22, 26, 26, 19, 26, 10, 21, 25,  6, 37, 33, 13, 28, 26, 16,\n",
      "        26, 15, 15, 37, 10, 38, 25, 14,  1,  8,  8, 16, 12, 16, 16,  8, 31, 28,\n",
      "        35, 35,  7, 12, 14, 34, 13, 23, 28, 29, 15, 15,  9, 22, 29, 30, 11, 33,\n",
      "        24,  8, 35, 34, 29, 25, 30, 10, 33, 38, 15, 20, 28, 37, 35,  3,  8, 35,\n",
      "         1, 31, 31, 33, 26, 31, 16,  5, 12, 35, 29, 17, 38, 11, 32,  6, 26, 26,\n",
      "        25,  5, 34,  3, 17, 18,  6, 12,  6,  6, 26, 35, 10, 11, 16, 23, 35,  5,\n",
      "         6, 26, 37, 14,  1, 28, 28, 16, 28, 28, 13, 13, 14, 18, 23,  3, 12, 36,\n",
      "         4, 25, 31, 19, 10, 31, 13, 13, 10, 12, 30, 16, 26, 31, 31,  3, 27, 11,\n",
      "         1, 30, 30,  6,  4, 33,  7,  3, 19, 32, 17, 17,  7, 30, 10,  7, 10, 17,\n",
      "        13,  9,  9, 27, 32, 19,  4, 10, 34, 36, 14, 17, 34, 22, 27,  9, 17, 22,\n",
      "        20, 35,  1], grad_fn=<NotImplemented>) \n",
      "sum:297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10200 0.002 0.037764277309179306 0.9878787878787879\n",
      "0 10400 0.002 0.018591322004795074 0.9970845481049563\n",
      "0 10600 0.002 0.19074751436710358 0.948948948948949\n",
      "0 10800 0.002 0.09645926207304001 0.9797101449275363\n",
      "0 11000 0.002 0.035375095903873444 0.990625\n",
      "0 11200 0.002 0.021488884463906288 0.9942528735632183\n",
      "0 11400 0.002 0.024565162137150764 0.9968454258675079\n",
      "0 11600 0.002 0.036881834268569946 0.99079754601227\n",
      "0 11800 0.002 0.030462488532066345 0.9907407407407407\n",
      "0 12000 0.002 0.07674966007471085 0.9795918367346939\n",
      "0 12200 0.002 0.068138986825943 0.9849397590361446\n",
      "0 12400 0.002 0.006951395887881517 1.0\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        #x = [8, 50]\n",
    "        #y = [8, 51] batchsize 为8\n",
    "        \n",
    "        # 在训练时,是拿y的每一个字符输入,预测下一个字符,所以不需要最后一个字\n",
    "        # [8, 50, 39]\n",
    "        pred = model(x, y[:, :-1])\n",
    "\n",
    "        #[8, 50, 39] -> [400, 39]\n",
    "        pred = pred.reshape(-1, 39)\n",
    "        \n",
    "        #[8, 51] -> [400, 39]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "\n",
    "        #忽略pad\n",
    "        select = y != zidian_y['<PAD>']\n",
    "        pred = pred[select]\n",
    "        y = y[select]\n",
    "        \n",
    "        loss = loss_func(pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            \n",
    "            pred = pred.argmax(1)\n",
    "            \n",
    "            correct = (pred == y).sum().item()\n",
    "            accuracy = correct / len(pred)\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(epoch, i, lr, loss.item(), accuracy)\n",
    "            if i %10000 == 0:\n",
    "                print(\"y:{}\\npred:{} \\nsum:{}\".format(y,pred,(pred == y).sum()))\n",
    "    sched.step()\n",
    "    \n",
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f58daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac8fde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    # x = [1, 50]\n",
    "    model.eval()\n",
    "\n",
    "    # [1, 1, 50, 50]\n",
    "    mask_pad_x = mask_pad(x)\n",
    "\n",
    "    # 初始化输出,这个是固定值\n",
    "    # [1, 50]\n",
    "    # [[0,2,2,2...]]\n",
    "    target = [zidian_y['<SOS>']] + [zidian_y['<PAD>']] * 49\n",
    "    target = torch.LongTensor(target).unsqueeze(0)\n",
    "\n",
    "    # x编码,添加位置信息\n",
    "    # [1, 50] -> [1, 50, 32]\n",
    "    x = model.embed_x(x)\n",
    "\n",
    "    # 编码层计算,维度不变\n",
    "    # [1, 50, 32] -> [1, 50, 32]\n",
    "    x = model.encoder(x, mask_pad_x)\n",
    "\n",
    "    # 遍历生成第1个词到第49个词\n",
    "    for i in range(49):\n",
    "        # [1, 50]\n",
    "        y = target\n",
    "\n",
    "        # [1, 1, 50, 50]\n",
    "        mask_tril_y = mask_tril(y)\n",
    "\n",
    "        # y编码,添加位置信息\n",
    "        # [1, 50] -> [1, 50, 32]\n",
    "        y = model.embed_y(y)\n",
    "\n",
    "        # 解码层计算,维度不变\n",
    "        # [1, 50, 32],[1, 50, 32] -> [1, 50, 32]\n",
    "        y = model.decoder(x, y, mask_pad_x, mask_tril_y)\n",
    "\n",
    "        # 全连接输出,39分类\n",
    "        # [1, 50, 32] -> [1, 50, 39]\n",
    "        out = model.fc_out(y)\n",
    "\n",
    "        # 取出当前词的输出\n",
    "        # [1, 50, 39] -> [1, 39]\n",
    "        out = out[:, i, :]\n",
    "\n",
    "        # 取出分类结果\n",
    "        # [1, 39] -> [1]\n",
    "        out = out.argmax(dim=1).detach()\n",
    "\n",
    "        # 以当前词预测下一个词,填到结果中\n",
    "        target[:, i + 1] = out\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0198606f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 37, 36,  7, 15, 16, 26,  8, 30,  8, 33, 25, 31,  3,  6, 34, 15, 15,\n",
      "        25, 19,  8, 25, 19, 31, 37, 10, 23, 23, 33, 36,  4, 34, 34, 16,  6, 25,\n",
      "         5,  3, 35, 19, 15, 22, 10,  1,  2,  2,  2,  2,  2,  2])\n",
      "<SOS>nb4erf5k5xdl03ceedu5duln7aaxb1ccr3d20vuep7<EOS><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "tensor([[ 0,  5,  5, 22, 15, 19, 35, 12, 10, 25,  9, 16, 34, 34, 11, 36, 33, 23,\n",
      "         23,  5, 37, 31, 19, 25,  7, 19, 25, 15, 15, 34,  9, 12, 31, 25, 33,  7,\n",
      "         30,  7, 26, 16, 15,  8, 36, 37,  1,  1,  1,  1,  1,  1]])\n",
      "<SOS>22PEUV97D6RCC8BXAA2NLUD4UDEEC69LDX4K4FRE5BN<EOS><EOS><EOS><EOS><EOS><EOS>\n"
     ]
    }
   ],
   "source": [
    "# x = [zidian_x['<SOS>']]+[zidian_x['m'],zidian_x['g'],zidian_x['d']] +[zidian_x['<EOS>']]+ [zidian_x['<PAD>']]*50\n",
    "x = ['<SOS>','m','f','j','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','c','<EOS>'] + ['<PAD>']*50\n",
    "x = x[:50]\n",
    "# print(x)\n",
    "x = [zidian_x[k] for k in x]\n",
    "x = [ 0, 37, 36,  7, 15, 16, 26,  8, 30,  8, 33, 25, 31,  3,  6, 34, 15, 15,\n",
    "         25, 19,  8, 25, 19, 31, 37, 10, 23, 23, 33, 36,  4, 34, 34, 16,  6, 25,\n",
    "          5,  3, 35, 19, 15, 22, 10,  1,  2,  2,  2,  2,  2,  2]\n",
    "\n",
    "x = torch.LongTensor(x)\n",
    "print(x)\n",
    "print(''.join([zidian_xr[i] for i in x.tolist()]))\n",
    "out = predict(x)\n",
    "print(out)\n",
    "print(''.join([zidian_yr[i] for i in out[i].unsqueeze(0)[0].tolist()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08f81d",
   "metadata": {},
   "source": [
    "该训练比较简单，若文本过短效果不好，只能找到长序列的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "494ac902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "#正式测试\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    print(type(i))\n",
    "    break\n",
    "    \n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    print(''.join([zidian_xr[i] for i in x[i].tolist()]))\n",
    "    print(''.join([zidian_yr[i] for i in y[i].tolist()]))\n",
    "    print(''.join([zidian_yr[i] for i in predict1(x[i].unsqueeze(0))[0].tolist()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
